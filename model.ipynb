{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1tKKp8JmrLaLnF1sKWEVrU_ZhF1KRGqFm","authorship_tag":"ABX9TyMR3w8Bd5nLFY5doXt/UjEw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fqFrGkW8VOcW"},"outputs":[],"source":["%cd drive/MyDrive/ECS289_final/\n","import gensim\n","from gensim.models import word2vec\n","from gensim.models import KeyedVectors\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import PIL\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import*\n","from tensorflow.keras.models import Sequential\n","\n","!pip install tensorflow_addons\n","import tensorflow_addons as tfa\n","from tensorflow.keras.layers import*\n","\n","class Seq2SeqWithAttention(tf.keras.Model):\n","    def __init__(self, enc_v_dim, dec_v_dim, emb_dim, units, attention_layer_size, max_pred_len, start_token, end_token):\n","        super().__init__()\n","\n","        EMBEDDING_FILE = '/content/drive/MyDrive/ECS289_final/dataset/GoogleNews-vectors-negative300.bin'\n","        self.word_matrix = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n","\n","        self.units = units\n","\n","        # encoder\n","        self.enc_embeddings = keras.layers.Embedding(\n","            input_dim=enc_v_dim, output_dim=emb_dim,    # [enc_n_vocab, emb_dim]\n","            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1)\n","        )\n","        self.encoder = keras.layers.LSTM(units=units, return_sequences=True, return_state=True)\n","\n","        # decoder\n","        self.attention = tfa.seq2seq.LuongAttention(\n","            units,    # Units is usde in dense function for computing e (scores)\n","            memory=None, \n","            memory_sequence_length=None) \n","            \n","        self.decoder_cell = tfa.seq2seq.AttentionWrapper(\n","            cell=keras.layers.LSTMCell(units=units),\n","            attention_mechanism=self.attention,\n","            attention_layer_size=None,\n","        )\n","\n","        self.dec_embeddings = keras.layers.Embedding(\n","            input_dim=dec_v_dim, output_dim=emb_dim,    # [dec_n_vocab, emb_dim]\n","            embeddings_initializer=tf.initializers.RandomNormal(0., 0.1),\n","        )\n","        decoder_dense = keras.layers.Dense(dec_v_dim)   # output layer\n","\n","        # train decoder\n","        self.decoder_train = tfa.seq2seq.BasicDecoder(\n","            cell=self.decoder_cell,\n","            sampler=tfa.seq2seq.sampler.TrainingSampler(),   # sampler for train\n","            output_layer=decoder_dense\n","        )\n","        self.cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        self.opt = keras.optimizers.Adam(0.05, clipnorm=5.0)\n","\n","        # predict decoder\n","        self.decoder_eval = tfa.seq2seq.BasicDecoder(\n","            cell=self.decoder_cell,\n","            sampler=tfa.seq2seq.sampler.GreedyEmbeddingSampler(),       # sampler for predict\n","            output_layer=decoder_dense\n","        )\n","\n","        # prediction restriction\n","        self.max_pred_len = max_pred_len\n","        self.start_token = start_token\n","        self.end_token = end_token\n","\n","    def encode(self, x):\n","        o = self.enc_embeddings(x)\n","        init_s = [tf.zeros((x.shape[0], self.units)), tf.zeros((x.shape[0], self.units))]\n","        \n","        # outputs (all hidden state of each time step), last hidden state(a), last cell state(c)\n","        o, h, c = self.encoder(o, initial_state=init_s)\n","        return o, h, c\n","\n","    def set_attention(self, x):\n","        # encoder output for attention to focus\n","        # o: all hidden states for computing attention\n","        o, h, c = self.encode(x)          \n","        self.attention.setup_memory(o)\n","        \n","        # wrap state by attention wrapper\n","        '''\n","        [h, c] is cell state of decoder(s0),\n","        and s contains contexts(named as attention inside funciton), alignments (attention at each time step) \n","        and alignment history.\n","        context = sum of alginments*hidden state from encoder(a)\n","        \n","        Then, it setup all hidden states (o) into attention object,\n","        and it initialize s0 of decoder with last [h, c] of encoder.\n","        \n","        After then, it could calculate scores (e) with s0 and o, then calculate attention (alpha) and names it alginments.\n","        Finally, it coudle get context named as attention inside of s.\n","        \n","        Moreover, we get s1, s2, ... at each time step and iterate the same step to get context c.\n","        '''\n","        s = self.decoder_cell.get_initial_state(batch_size=x.shape[0], dtype=tf.float32).clone(cell_state=[h, c])\n","        return s, o\n","\n","    def inference(self, x):\n","        s, _ = self.set_attention(x)\n","        \n","        # s includes hidden state from docoder(s) and context(c)\n","        done, i, s = self.decoder_eval.initialize(\n","            self.dec_embeddings.variables[0],\n","            start_tokens=tf.fill([x.shape[0], ], self.start_token),\n","            end_token=self.end_token,\n","            initial_state=s,\n","        )\n","        \n","        pred_id = np.zeros((x.shape[0], self.max_pred_len), dtype=np.int32)\n","        for l in range(self.max_pred_len):\n","            o, s, i, done = self.decoder_eval.step(\n","                time=l, inputs=i, state=s, training=False)\n","            \n","            pred_id[:, l] = o.sample_id\n","            \n","            '''\n","            For prove s contains context(called attention) and attention (called alginments)\n","\n","            c = tf.tensordot(tf.reshape(s[2][0, :], [1, -1]), _[0, :], axes=1)\n","            print(c == s[1])\n","            print(c.shape)\n","            print(s[1].shape)\n","            '''\n","\n","        return pred_id\n","\n","    def train_logits(self, x, y, seq_len):\n","        s,_ = self.set_attention(x)\n","        \n","        dec_in = y[:, :-1]   # ignore <EOS>\n","        dec_emb_in = self.dec_embeddings(dec_in)\n","        \n","        o, _, _ = self.decoder_train(dec_emb_in, s, sequence_length=seq_len)\n","        logits = o.rnn_output\n","        return logits\n","\n","    def step(self, x, y, seq_len):\n","        \n","        with tf.GradientTape() as tape:\n","            logits = self.train_logits(x, y, seq_len)\n","            \n","            #print(logits.shape)\n","            dec_out = y[:, 1:]  # ignore <GO>\n","            \n","            loss = self.cross_entropy(dec_out, logits)\n","            grads = tape.gradient(loss, self.trainable_variables)\n","            \n","        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n","        return loss.numpy()"]},{"cell_type":"code","source":["model = VQAModel(len(ans_vocab), len(ques_vocab))"],"metadata":{"id":"Ces92GHhmRJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7EXEM-ntsT70"},"execution_count":null,"outputs":[]}]}